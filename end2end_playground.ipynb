{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bnlZQ0EOE4xGe4jYAC-2o2EfIbgPUZUu",
      "authorship_tag": "ABX9TyOFVlKJQdyI8QkZdL4VDUMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmyxth/2023-Utah/blob/main/end2end_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sympy==1.11\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install tabulate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIiDbsGlfuN4",
        "outputId": "7fd5cb2d-adf4-4084-afba-59579fb30cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy==1.11 in /usr/local/lib/python3.10/dist-packages (1.11)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy==1.11) (1.3.0)\n",
            "Requirement already satisfied: torch==2.0 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (1.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0) (3.31.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch==2.5.1 (from torchvision)\n",
            "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->torchvision)\n",
            "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1->torchvision)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.0.0\n",
            "    Uninstalling triton-2.0.0:\n",
            "      Successfully uninstalled triton-2.0.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.11\n",
            "    Uninstalling sympy-1.11:\n",
            "      Successfully uninstalled sympy-1.11\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.24.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.24.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.24.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0\n",
            "    Uninstalling torch-2.0.0:\n",
            "      Successfully uninstalled torch-2.0.0\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1 triton-3.1.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Collecting sympy==1.11\n",
            "  Using cached sympy-1.11-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy==1.11) (1.3.0)\n",
            "Using cached sympy-1.11-py3-none-any.whl (6.5 MB)\n",
            "Installing collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.11 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.11\n",
            "Collecting torch==2.0\n",
            "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (1.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0) (11.7.91)\n",
            "Collecting triton==2.0.0 (from torch==2.0)\n",
            "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0) (3.31.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0) (1.3.0)\n",
            "Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Installing collected packages: triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1\n",
            "    Uninstalling torch-2.5.1:\n",
            "      Successfully uninstalled torch-2.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.0 triton-2.0.0\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Collecting torch==2.5.1 (from torchvision)\n",
            "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1->torchvision)\n",
            "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1->torchvision)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Installing collected packages: triton, sympy, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.0.0\n",
            "    Uninstalling triton-2.0.0:\n",
            "      Successfully uninstalled triton-2.0.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.11\n",
            "    Uninstalling sympy-1.11:\n",
            "      Successfully uninstalled sympy-1.11\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0\n",
            "    Uninstalling torch-2.0.0:\n",
            "      Successfully uninstalled torch-2.0.0\n",
            "Successfully installed sympy-1.13.1 torch-2.5.1 triton-3.1.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision==0.19"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "78FRkXoukYpK",
        "outputId": "5ae08efa-625a-48e5-a111-616589ad80a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchvision==0.19\n",
            "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19) (1.26.4)\n",
            "Collecting torch==2.4.0 (from torchvision==0.19)\n",
            "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision==0.19) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision==0.19) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision==0.19) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision==0.19) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision==0.19) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision==0.19) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision==0.19) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0->torchvision==0.19)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->torchvision==0.19) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchvision==0.19) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchvision==0.19) (1.3.0)\n",
            "Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m872.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1\n",
            "    Uninstalling torch-2.5.1:\n",
            "      Successfully uninstalled torch-2.5.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchvision-0.19.0 triton-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "functorch",
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "42358344ccab4c269d1452ee79079966"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch;\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dfc-DP_ugJQA",
        "outputId": "0d798f88-92a0-4650-e620-7c4b6adac2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install depyf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc8mesb2nx1Y",
        "outputId": "2eb114a8-d49b-4c6e-fe59-0a331fef3c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting depyf\n",
            "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting astor (from depyf)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting dill (from depyf)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill, astor, depyf\n",
            "Successfully installed astor-0.8.1 depyf-0.18.0 dill-0.3.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import depyf\n",
        "depyf.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "Boq66KTWn9lP",
        "outputId": "131d7bb6-6c5a-412e-ce99-5e4fbb4253d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'depyf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-87cdd0585a26>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdepyf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdepyf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'depyf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lGXOo5SUenU4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def fn(x):\n",
        "   a = torch.cos(x)\n",
        "   b = torch.sin(a)\n",
        "   return b\n",
        "new_fn = torch.compile(fn, backend=\"inductor\")\n",
        "input_tensor = torch.randn(10000)\n",
        "a = new_fn(input_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.fx.passes.graph_drawer import FxGraphDrawer\n",
        "\n",
        "def dynamo_backend(gm, sample_inputs):\n",
        "    code = gm.print_readable()\n",
        "    gm.graph.print_tabular()\n",
        "    with open(\"transformer_graph.svg\", \"wb\") as file:\n",
        "        file.write(FxGraphDrawer(gm,'f').get_dot_graph().create_svg())\n",
        "    return gm.forward"
      ],
      "metadata": {
        "id": "RNiJYTnPtPDJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bo3-iBplvozb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "# Copy pasted from here https://huggingface.co/bert-base-uncased\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model = torch.compile(model, backend=dynamo_backend)\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEme9wjGjh8b",
        "outputId": "f9f4d31a-8d07-4c38-f99b-09726fda19da"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class GraphModule(torch.nn.Module):\n",
            "    def forward(self, L_input_ids_: \"i64[1, 12]\", L_token_type_ids_: \"i64[1, 12]\", L_self_modules_embeddings_buffers_position_ids_: \"i64[1, 512]\", L_self_modules_embeddings_modules_word_embeddings_parameters_weight_: \"f32[30522, 768]\", L_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_: \"f32[2, 768]\", L_self_modules_embeddings_modules_position_embeddings_parameters_weight_: \"f32[512, 768]\", L_self_modules_embeddings_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_embeddings_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_attention_mask_: \"i64[1, 12]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_: \"f32[3072, 768]\", L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_: \"f32[3072]\", L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_: \"f32[768, 3072]\", L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_: \"f32[768]\", L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_: \"f32[768]\", L_self_modules_pooler_modules_dense_parameters_weight_: \"f32[768, 768]\", L_self_modules_pooler_modules_dense_parameters_bias_: \"f32[768]\"):\n",
            "        l_input_ids_ = L_input_ids_\n",
            "        l_token_type_ids_ = L_token_type_ids_\n",
            "        l_self_modules_embeddings_buffers_position_ids_ = L_self_modules_embeddings_buffers_position_ids_\n",
            "        l_self_modules_embeddings_modules_word_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_word_embeddings_parameters_weight_\n",
            "        l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_\n",
            "        l_self_modules_embeddings_modules_position_embeddings_parameters_weight_ = L_self_modules_embeddings_modules_position_embeddings_parameters_weight_\n",
            "        l_self_modules_embeddings_modules_layer_norm_parameters_weight_ = L_self_modules_embeddings_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_embeddings_modules_layer_norm_parameters_bias_ = L_self_modules_embeddings_modules_LayerNorm_parameters_bias_\n",
            "        l_attention_mask_ = L_attention_mask_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_\n",
            "        l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_ = L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_\n",
            "        l_self_modules_pooler_modules_dense_parameters_weight_ = L_self_modules_pooler_modules_dense_parameters_weight_\n",
            "        l_self_modules_pooler_modules_dense_parameters_bias_ = L_self_modules_pooler_modules_dense_parameters_bias_\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:197 in forward, code: position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
            "        position_ids: \"i64[1, 12]\" = l_self_modules_embeddings_buffers_position_ids_[(slice(None, None, None), slice(0, 12, None))];  l_self_modules_embeddings_buffers_position_ids_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:211 in forward, code: inputs_embeds = self.word_embeddings(input_ids)\n",
            "        inputs_embeds: \"f32[1, 12, 768]\" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_embeddings_modules_word_embeddings_parameters_weight_, 0, None, 2.0, False, False);  l_input_ids_ = l_self_modules_embeddings_modules_word_embeddings_parameters_weight_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:212 in forward, code: token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
            "        token_type_embeddings: \"f32[1, 12, 768]\" = torch.nn.functional.embedding(l_token_type_ids_, l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_token_type_ids_ = l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:214 in forward, code: embeddings = inputs_embeds + token_type_embeddings\n",
            "        embeddings: \"f32[1, 12, 768]\" = inputs_embeds + token_type_embeddings;  inputs_embeds = token_type_embeddings = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:216 in forward, code: position_embeddings = self.position_embeddings(position_ids)\n",
            "        position_embeddings: \"f32[1, 12, 768]\" = torch.nn.functional.embedding(position_ids, l_self_modules_embeddings_modules_position_embeddings_parameters_weight_, None, None, 2.0, False, False);  position_ids = l_self_modules_embeddings_modules_position_embeddings_parameters_weight_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:217 in forward, code: embeddings += position_embeddings\n",
            "        embeddings += position_embeddings;  embeddings_1: \"f32[1, 12, 768]\" = embeddings;  embeddings = position_embeddings = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:218 in forward, code: embeddings = self.LayerNorm(embeddings)\n",
            "        embeddings_2: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(embeddings_1, (768,), l_self_modules_embeddings_modules_layer_norm_parameters_weight_, l_self_modules_embeddings_modules_layer_norm_parameters_bias_, 1e-12);  embeddings_1 = l_self_modules_embeddings_modules_layer_norm_parameters_weight_ = l_self_modules_embeddings_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:219 in forward, code: embeddings = self.dropout(embeddings)\n",
            "        embeddings_3: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(embeddings_2, 0.1, False, False);  embeddings_2 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:184 in _expand_mask, code: expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
            "        getitem_1: \"i64[1, 1, 1, 12]\" = l_attention_mask_[(slice(None, None, None), None, None, slice(None, None, None))];  l_attention_mask_ = None\n",
            "        expand: \"i64[1, 1, 12, 12]\" = getitem_1.expand(1, 1, 12, 12);  getitem_1 = None\n",
            "        expanded_mask: \"f32[1, 1, 12, 12]\" = expand.to(torch.float32);  expand = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:186 in _expand_mask, code: inverted_mask = 1.0 - expanded_mask\n",
            "        inverted_mask: \"f32[1, 1, 12, 12]\" = 1.0 - expanded_mask;  expanded_mask = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:188 in _expand_mask, code: return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
            "        to_1: \"b8[1, 1, 12, 12]\" = inverted_mask.to(torch.bool)\n",
            "        extended_attention_mask: \"f32[1, 1, 12, 12]\" = inverted_mask.masked_fill(to_1, -3.4028234663852886e+38);  inverted_mask = to_1 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear: \"f32[1, 12, 768]\" = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x: \"f32[1, 12, 12, 64]\" = linear.view((1, 12, 12, 64));  linear = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer: \"f32[1, 12, 12, 64]\" = x.permute(0, 2, 1, 3);  x = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_1: \"f32[1, 12, 768]\" = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_1: \"f32[1, 12, 12, 64]\" = linear_1.view((1, 12, 12, 64));  linear_1 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer: \"f32[1, 12, 12, 64]\" = x_1.permute(0, 2, 1, 3);  x_1 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_2: \"f32[1, 12, 768]\" = torch._C._nn.linear(embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_2: \"f32[1, 12, 12, 64]\" = linear_2.view((1, 12, 12, 64));  linear_2 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer: \"f32[1, 12, 12, 64]\" = x_2.permute(0, 2, 1, 3);  x_2 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer, key_layer, value_layer, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer = key_layer = value_layer = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_1: \"f32[1, 12, 12, 64]\" = attn_output.transpose(1, 2);  attn_output = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_2: \"f32[1, 12, 768]\" = attn_output_1.reshape(1, 12, 768);  attn_output_1 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_2, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_2 = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_1: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states, 0.1, False, False);  hidden_states = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_1: \"f32[1, 12, 768]\" = hidden_states_1 + embeddings_3;  hidden_states_1 = embeddings_3 = None\n",
            "        hidden_states_2: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_1, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_1 = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_3: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_2, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_4: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_3);  hidden_states_3 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_5: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_4, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_);  hidden_states_4 = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_6: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_5, 0.1, False, False);  hidden_states_5 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_2: \"f32[1, 12, 768]\" = hidden_states_6 + hidden_states_2;  hidden_states_6 = hidden_states_2 = None\n",
            "        hidden_states_7: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_2, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_2 = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_6: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_3: \"f32[1, 12, 12, 64]\" = linear_6.view((1, 12, 12, 64));  linear_6 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_1: \"f32[1, 12, 12, 64]\" = x_3.permute(0, 2, 1, 3);  x_3 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_7: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_4: \"f32[1, 12, 12, 64]\" = linear_7.view((1, 12, 12, 64));  linear_7 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_1: \"f32[1, 12, 12, 64]\" = x_4.permute(0, 2, 1, 3);  x_4 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_8: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_5: \"f32[1, 12, 12, 64]\" = linear_8.view((1, 12, 12, 64));  linear_8 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_1: \"f32[1, 12, 12, 64]\" = x_5.permute(0, 2, 1, 3);  x_5 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_3: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_1, key_layer_1, value_layer_1, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_1 = key_layer_1 = value_layer_1 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_4: \"f32[1, 12, 12, 64]\" = attn_output_3.transpose(1, 2);  attn_output_3 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_5: \"f32[1, 12, 768]\" = attn_output_4.reshape(1, 12, 768);  attn_output_4 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_8: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_5, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_5 = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_9: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_8, 0.1, False, False);  hidden_states_8 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_3: \"f32[1, 12, 768]\" = hidden_states_9 + hidden_states_7;  hidden_states_9 = hidden_states_7 = None\n",
            "        hidden_states_10: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_3, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_3 = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_11: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_10, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_12: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_11);  hidden_states_11 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_13: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_12, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_);  hidden_states_12 = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_14: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_13, 0.1, False, False);  hidden_states_13 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_4: \"f32[1, 12, 768]\" = hidden_states_14 + hidden_states_10;  hidden_states_14 = hidden_states_10 = None\n",
            "        hidden_states_15: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_4, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_4 = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_12: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_6: \"f32[1, 12, 12, 64]\" = linear_12.view((1, 12, 12, 64));  linear_12 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_2: \"f32[1, 12, 12, 64]\" = x_6.permute(0, 2, 1, 3);  x_6 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_13: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_7: \"f32[1, 12, 12, 64]\" = linear_13.view((1, 12, 12, 64));  linear_13 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_2: \"f32[1, 12, 12, 64]\" = x_7.permute(0, 2, 1, 3);  x_7 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_14: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_8: \"f32[1, 12, 12, 64]\" = linear_14.view((1, 12, 12, 64));  linear_14 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_2: \"f32[1, 12, 12, 64]\" = x_8.permute(0, 2, 1, 3);  x_8 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_6: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_2, key_layer_2, value_layer_2, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_2 = key_layer_2 = value_layer_2 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_7: \"f32[1, 12, 12, 64]\" = attn_output_6.transpose(1, 2);  attn_output_6 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_8: \"f32[1, 12, 768]\" = attn_output_7.reshape(1, 12, 768);  attn_output_7 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_16: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_8, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_8 = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_17: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_16, 0.1, False, False);  hidden_states_16 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_5: \"f32[1, 12, 768]\" = hidden_states_17 + hidden_states_15;  hidden_states_17 = hidden_states_15 = None\n",
            "        hidden_states_18: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_5, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_5 = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_19: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_18, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_20: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_19);  hidden_states_19 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_21: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_20, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_);  hidden_states_20 = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_22: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_21, 0.1, False, False);  hidden_states_21 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_6: \"f32[1, 12, 768]\" = hidden_states_22 + hidden_states_18;  hidden_states_22 = hidden_states_18 = None\n",
            "        hidden_states_23: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_6, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_6 = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_18: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_9: \"f32[1, 12, 12, 64]\" = linear_18.view((1, 12, 12, 64));  linear_18 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_3: \"f32[1, 12, 12, 64]\" = x_9.permute(0, 2, 1, 3);  x_9 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_19: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_10: \"f32[1, 12, 12, 64]\" = linear_19.view((1, 12, 12, 64));  linear_19 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_3: \"f32[1, 12, 12, 64]\" = x_10.permute(0, 2, 1, 3);  x_10 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_20: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_11: \"f32[1, 12, 12, 64]\" = linear_20.view((1, 12, 12, 64));  linear_20 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_3: \"f32[1, 12, 12, 64]\" = x_11.permute(0, 2, 1, 3);  x_11 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_9: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_3, key_layer_3, value_layer_3, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_3 = key_layer_3 = value_layer_3 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_10: \"f32[1, 12, 12, 64]\" = attn_output_9.transpose(1, 2);  attn_output_9 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_11: \"f32[1, 12, 768]\" = attn_output_10.reshape(1, 12, 768);  attn_output_10 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_24: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_11, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_11 = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_25: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_24, 0.1, False, False);  hidden_states_24 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_7: \"f32[1, 12, 768]\" = hidden_states_25 + hidden_states_23;  hidden_states_25 = hidden_states_23 = None\n",
            "        hidden_states_26: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_7, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_7 = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_27: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_26, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_28: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_27);  hidden_states_27 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_29: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_28, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_);  hidden_states_28 = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_30: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_29, 0.1, False, False);  hidden_states_29 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_8: \"f32[1, 12, 768]\" = hidden_states_30 + hidden_states_26;  hidden_states_30 = hidden_states_26 = None\n",
            "        hidden_states_31: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_8, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_8 = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_24: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_12: \"f32[1, 12, 12, 64]\" = linear_24.view((1, 12, 12, 64));  linear_24 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_4: \"f32[1, 12, 12, 64]\" = x_12.permute(0, 2, 1, 3);  x_12 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_25: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_13: \"f32[1, 12, 12, 64]\" = linear_25.view((1, 12, 12, 64));  linear_25 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_4: \"f32[1, 12, 12, 64]\" = x_13.permute(0, 2, 1, 3);  x_13 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_26: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_14: \"f32[1, 12, 12, 64]\" = linear_26.view((1, 12, 12, 64));  linear_26 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_4: \"f32[1, 12, 12, 64]\" = x_14.permute(0, 2, 1, 3);  x_14 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_12: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_4, key_layer_4, value_layer_4, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_4 = key_layer_4 = value_layer_4 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_13: \"f32[1, 12, 12, 64]\" = attn_output_12.transpose(1, 2);  attn_output_12 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_14: \"f32[1, 12, 768]\" = attn_output_13.reshape(1, 12, 768);  attn_output_13 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_32: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_14, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_14 = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_33: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_32, 0.1, False, False);  hidden_states_32 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_9: \"f32[1, 12, 768]\" = hidden_states_33 + hidden_states_31;  hidden_states_33 = hidden_states_31 = None\n",
            "        hidden_states_34: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_9, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_9 = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_35: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_34, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_36: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_35);  hidden_states_35 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_37: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_36, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_);  hidden_states_36 = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_38: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_37, 0.1, False, False);  hidden_states_37 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_10: \"f32[1, 12, 768]\" = hidden_states_38 + hidden_states_34;  hidden_states_38 = hidden_states_34 = None\n",
            "        hidden_states_39: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_10, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_10 = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_30: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_15: \"f32[1, 12, 12, 64]\" = linear_30.view((1, 12, 12, 64));  linear_30 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_5: \"f32[1, 12, 12, 64]\" = x_15.permute(0, 2, 1, 3);  x_15 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_31: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_16: \"f32[1, 12, 12, 64]\" = linear_31.view((1, 12, 12, 64));  linear_31 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_5: \"f32[1, 12, 12, 64]\" = x_16.permute(0, 2, 1, 3);  x_16 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_32: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_17: \"f32[1, 12, 12, 64]\" = linear_32.view((1, 12, 12, 64));  linear_32 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_5: \"f32[1, 12, 12, 64]\" = x_17.permute(0, 2, 1, 3);  x_17 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_15: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_5, key_layer_5, value_layer_5, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_5 = key_layer_5 = value_layer_5 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_16: \"f32[1, 12, 12, 64]\" = attn_output_15.transpose(1, 2);  attn_output_15 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_17: \"f32[1, 12, 768]\" = attn_output_16.reshape(1, 12, 768);  attn_output_16 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_40: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_17, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_17 = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_41: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_40, 0.1, False, False);  hidden_states_40 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_11: \"f32[1, 12, 768]\" = hidden_states_41 + hidden_states_39;  hidden_states_41 = hidden_states_39 = None\n",
            "        hidden_states_42: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_11, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_11 = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_43: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_42, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_44: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_43);  hidden_states_43 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_45: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_44, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_);  hidden_states_44 = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_46: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_45, 0.1, False, False);  hidden_states_45 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_12: \"f32[1, 12, 768]\" = hidden_states_46 + hidden_states_42;  hidden_states_46 = hidden_states_42 = None\n",
            "        hidden_states_47: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_12, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_12 = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_36: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_18: \"f32[1, 12, 12, 64]\" = linear_36.view((1, 12, 12, 64));  linear_36 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_6: \"f32[1, 12, 12, 64]\" = x_18.permute(0, 2, 1, 3);  x_18 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_37: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_19: \"f32[1, 12, 12, 64]\" = linear_37.view((1, 12, 12, 64));  linear_37 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_6: \"f32[1, 12, 12, 64]\" = x_19.permute(0, 2, 1, 3);  x_19 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_38: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_20: \"f32[1, 12, 12, 64]\" = linear_38.view((1, 12, 12, 64));  linear_38 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_6: \"f32[1, 12, 12, 64]\" = x_20.permute(0, 2, 1, 3);  x_20 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_18: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_6, key_layer_6, value_layer_6, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_6 = key_layer_6 = value_layer_6 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_19: \"f32[1, 12, 12, 64]\" = attn_output_18.transpose(1, 2);  attn_output_18 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_20: \"f32[1, 12, 768]\" = attn_output_19.reshape(1, 12, 768);  attn_output_19 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_48: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_20, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_20 = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_49: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_48, 0.1, False, False);  hidden_states_48 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_13: \"f32[1, 12, 768]\" = hidden_states_49 + hidden_states_47;  hidden_states_49 = hidden_states_47 = None\n",
            "        hidden_states_50: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_13, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_13 = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_51: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_50, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_52: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_51);  hidden_states_51 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_53: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_52, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_);  hidden_states_52 = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_54: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_53, 0.1, False, False);  hidden_states_53 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_14: \"f32[1, 12, 768]\" = hidden_states_54 + hidden_states_50;  hidden_states_54 = hidden_states_50 = None\n",
            "        hidden_states_55: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_14, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_14 = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_42: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_21: \"f32[1, 12, 12, 64]\" = linear_42.view((1, 12, 12, 64));  linear_42 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_7: \"f32[1, 12, 12, 64]\" = x_21.permute(0, 2, 1, 3);  x_21 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_43: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_22: \"f32[1, 12, 12, 64]\" = linear_43.view((1, 12, 12, 64));  linear_43 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_7: \"f32[1, 12, 12, 64]\" = x_22.permute(0, 2, 1, 3);  x_22 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_44: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_23: \"f32[1, 12, 12, 64]\" = linear_44.view((1, 12, 12, 64));  linear_44 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_7: \"f32[1, 12, 12, 64]\" = x_23.permute(0, 2, 1, 3);  x_23 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_21: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_7, key_layer_7, value_layer_7, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_7 = key_layer_7 = value_layer_7 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_22: \"f32[1, 12, 12, 64]\" = attn_output_21.transpose(1, 2);  attn_output_21 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_23: \"f32[1, 12, 768]\" = attn_output_22.reshape(1, 12, 768);  attn_output_22 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_56: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_23, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_23 = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_57: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_56, 0.1, False, False);  hidden_states_56 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_15: \"f32[1, 12, 768]\" = hidden_states_57 + hidden_states_55;  hidden_states_57 = hidden_states_55 = None\n",
            "        hidden_states_58: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_15, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_15 = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_59: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_58, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_60: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_59);  hidden_states_59 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_61: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_60, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_);  hidden_states_60 = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_62: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_61, 0.1, False, False);  hidden_states_61 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_16: \"f32[1, 12, 768]\" = hidden_states_62 + hidden_states_58;  hidden_states_62 = hidden_states_58 = None\n",
            "        hidden_states_63: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_16, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_16 = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_48: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_24: \"f32[1, 12, 12, 64]\" = linear_48.view((1, 12, 12, 64));  linear_48 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_8: \"f32[1, 12, 12, 64]\" = x_24.permute(0, 2, 1, 3);  x_24 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_49: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_25: \"f32[1, 12, 12, 64]\" = linear_49.view((1, 12, 12, 64));  linear_49 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_8: \"f32[1, 12, 12, 64]\" = x_25.permute(0, 2, 1, 3);  x_25 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_50: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_26: \"f32[1, 12, 12, 64]\" = linear_50.view((1, 12, 12, 64));  linear_50 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_8: \"f32[1, 12, 12, 64]\" = x_26.permute(0, 2, 1, 3);  x_26 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_24: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_8, key_layer_8, value_layer_8, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_8 = key_layer_8 = value_layer_8 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_25: \"f32[1, 12, 12, 64]\" = attn_output_24.transpose(1, 2);  attn_output_24 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_26: \"f32[1, 12, 768]\" = attn_output_25.reshape(1, 12, 768);  attn_output_25 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_64: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_26, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_26 = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_65: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_64, 0.1, False, False);  hidden_states_64 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_17: \"f32[1, 12, 768]\" = hidden_states_65 + hidden_states_63;  hidden_states_65 = hidden_states_63 = None\n",
            "        hidden_states_66: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_17, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_17 = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_67: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_66, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_68: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_67);  hidden_states_67 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_69: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_68, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_);  hidden_states_68 = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_70: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_69, 0.1, False, False);  hidden_states_69 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_18: \"f32[1, 12, 768]\" = hidden_states_70 + hidden_states_66;  hidden_states_70 = hidden_states_66 = None\n",
            "        hidden_states_71: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_18, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_18 = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_54: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_27: \"f32[1, 12, 12, 64]\" = linear_54.view((1, 12, 12, 64));  linear_54 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_9: \"f32[1, 12, 12, 64]\" = x_27.permute(0, 2, 1, 3);  x_27 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_55: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_28: \"f32[1, 12, 12, 64]\" = linear_55.view((1, 12, 12, 64));  linear_55 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_9: \"f32[1, 12, 12, 64]\" = x_28.permute(0, 2, 1, 3);  x_28 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_56: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_29: \"f32[1, 12, 12, 64]\" = linear_56.view((1, 12, 12, 64));  linear_56 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_9: \"f32[1, 12, 12, 64]\" = x_29.permute(0, 2, 1, 3);  x_29 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_27: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_9, key_layer_9, value_layer_9, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_9 = key_layer_9 = value_layer_9 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_28: \"f32[1, 12, 12, 64]\" = attn_output_27.transpose(1, 2);  attn_output_27 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_29: \"f32[1, 12, 768]\" = attn_output_28.reshape(1, 12, 768);  attn_output_28 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_72: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_29, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_29 = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_73: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_72, 0.1, False, False);  hidden_states_72 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_19: \"f32[1, 12, 768]\" = hidden_states_73 + hidden_states_71;  hidden_states_73 = hidden_states_71 = None\n",
            "        hidden_states_74: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_19, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_19 = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_75: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_74, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_76: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_75);  hidden_states_75 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_77: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_76, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_);  hidden_states_76 = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_78: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_77, 0.1, False, False);  hidden_states_77 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_20: \"f32[1, 12, 768]\" = hidden_states_78 + hidden_states_74;  hidden_states_78 = hidden_states_74 = None\n",
            "        hidden_states_79: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_20, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_20 = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_60: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_30: \"f32[1, 12, 12, 64]\" = linear_60.view((1, 12, 12, 64));  linear_60 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_10: \"f32[1, 12, 12, 64]\" = x_30.permute(0, 2, 1, 3);  x_30 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_61: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_31: \"f32[1, 12, 12, 64]\" = linear_61.view((1, 12, 12, 64));  linear_61 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_10: \"f32[1, 12, 12, 64]\" = x_31.permute(0, 2, 1, 3);  x_31 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_62: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_32: \"f32[1, 12, 12, 64]\" = linear_62.view((1, 12, 12, 64));  linear_62 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_10: \"f32[1, 12, 12, 64]\" = x_32.permute(0, 2, 1, 3);  x_32 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_30: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_10, key_layer_10, value_layer_10, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_10 = key_layer_10 = value_layer_10 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_31: \"f32[1, 12, 12, 64]\" = attn_output_30.transpose(1, 2);  attn_output_30 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_32: \"f32[1, 12, 768]\" = attn_output_31.reshape(1, 12, 768);  attn_output_31 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_80: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_32, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_32 = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_81: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_80, 0.1, False, False);  hidden_states_80 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_21: \"f32[1, 12, 768]\" = hidden_states_81 + hidden_states_79;  hidden_states_81 = hidden_states_79 = None\n",
            "        hidden_states_82: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_21, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_21 = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_83: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_82, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_84: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_83);  hidden_states_83 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_85: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_84, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_);  hidden_states_84 = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_86: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_85, 0.1, False, False);  hidden_states_85 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_22: \"f32[1, 12, 768]\" = hidden_states_86 + hidden_states_82;  hidden_states_86 = hidden_states_82 = None\n",
            "        hidden_states_87: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_22, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_22 = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:395 in forward, code: query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
            "        linear_66: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_33: \"f32[1, 12, 12, 64]\" = linear_66.view((1, 12, 12, 64));  linear_66 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        query_layer_11: \"f32[1, 12, 12, 64]\" = x_33.permute(0, 2, 1, 3);  x_33 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:408 in forward, code: key_layer = self.transpose_for_scores(self.key(current_states))\n",
            "        linear_67: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_34: \"f32[1, 12, 12, 64]\" = linear_67.view((1, 12, 12, 64));  linear_67 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        key_layer_11: \"f32[1, 12, 12, 64]\" = x_34.permute(0, 2, 1, 3);  x_34 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:409 in forward, code: value_layer = self.transpose_for_scores(self.value(current_states))\n",
            "        linear_68: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:252 in transpose_for_scores, code: x = x.view(new_x_shape)\n",
            "        x_35: \"f32[1, 12, 12, 64]\" = linear_68.view((1, 12, 12, 64));  linear_68 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:253 in transpose_for_scores, code: return x.permute(0, 2, 1, 3)\n",
            "        value_layer_11: \"f32[1, 12, 12, 64]\" = x_35.permute(0, 2, 1, 3);  x_35 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:440 in forward, code: attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "        attn_output_33: \"f32[1, 12, 12, 64]\" = torch._C._nn.scaled_dot_product_attention(query_layer_11, key_layer_11, value_layer_11, attn_mask = extended_attention_mask, dropout_p = 0.0, is_causal = False);  query_layer_11 = key_layer_11 = value_layer_11 = extended_attention_mask = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:449 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
            "        attn_output_34: \"f32[1, 12, 12, 64]\" = attn_output_33.transpose(1, 2);  attn_output_33 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:450 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
            "        attn_output_35: \"f32[1, 12, 768]\" = attn_output_34.reshape(1, 12, 768);  attn_output_34 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:466 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_88: \"f32[1, 12, 768]\" = torch._C._nn.linear(attn_output_35, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_);  attn_output_35 = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:467 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_89: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_88, 0.1, False, False);  hidden_states_88 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:468 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_23: \"f32[1, 12, 768]\" = hidden_states_89 + hidden_states_87;  hidden_states_89 = hidden_states_87 = None\n",
            "        hidden_states_90: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_23, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_23 = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:539 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_91: \"f32[1, 12, 3072]\" = torch._C._nn.linear(hidden_states_90, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_);  l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/activations.py:78 in forward, code: return self.act(input)\n",
            "        hidden_states_92: \"f32[1, 12, 3072]\" = torch._C._nn.gelu(hidden_states_91);  hidden_states_91 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:552 in forward, code: hidden_states = self.dense(hidden_states)\n",
            "        hidden_states_93: \"f32[1, 12, 768]\" = torch._C._nn.linear(hidden_states_92, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_);  hidden_states_92 = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:553 in forward, code: hidden_states = self.dropout(hidden_states)\n",
            "        hidden_states_94: \"f32[1, 12, 768]\" = torch.nn.functional.dropout(hidden_states_93, 0.1, False, False);  hidden_states_93 = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:554 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
            "        add_24: \"f32[1, 12, 768]\" = hidden_states_94 + hidden_states_90;  hidden_states_94 = hidden_states_90 = None\n",
            "        hidden_states_95: \"f32[1, 12, 768]\" = torch.nn.functional.layer_norm(add_24, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_, 1e-12);  add_24 = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_ = l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:746 in forward, code: first_token_tensor = hidden_states[:, 0]\n",
            "        first_token_tensor: \"f32[1, 768]\" = hidden_states_95[(slice(None, None, None), 0)]\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:747 in forward, code: pooled_output = self.dense(first_token_tensor)\n",
            "        pooled_output: \"f32[1, 768]\" = torch._C._nn.linear(first_token_tensor, l_self_modules_pooler_modules_dense_parameters_weight_, l_self_modules_pooler_modules_dense_parameters_bias_);  first_token_tensor = l_self_modules_pooler_modules_dense_parameters_weight_ = l_self_modules_pooler_modules_dense_parameters_bias_ = None\n",
            "        \n",
            "         # File: /usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:748 in forward, code: pooled_output = self.activation(pooled_output)\n",
            "        pooled_output_1: \"f32[1, 768]\" = torch.tanh(pooled_output);  pooled_output = None\n",
            "        return (hidden_states_95, pooled_output_1)\n",
            "        \n",
            "opcode         name                                                                                                                    target                                                                                                                 args                                                                                                                                                                                                                                                                   kwargs\n",
            "-------------  ----------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------\n",
            "placeholder    l_input_ids_                                                                                                            L_input_ids_                                                                                                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_token_type_ids_                                                                                                       L_token_type_ids_                                                                                                      ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_embeddings_buffers_position_ids_                                                                         L_self_modules_embeddings_buffers_position_ids_                                                                        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_embeddings_modules_word_embeddings_parameters_weight_                                                    L_self_modules_embeddings_modules_word_embeddings_parameters_weight_                                                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_                                              L_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_                                             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_embeddings_modules_position_embeddings_parameters_weight_                                                L_self_modules_embeddings_modules_position_embeddings_parameters_weight_                                               ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_embeddings_modules_layer_norm_parameters_weight_                                                         L_self_modules_embeddings_modules_LayerNorm_parameters_weight_                                                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_embeddings_modules_layer_norm_parameters_bias_                                                           L_self_modules_embeddings_modules_LayerNorm_parameters_bias_                                                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_attention_mask_                                                                                                       L_attention_mask_                                                                                                      ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_0_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_1_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_2_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_3_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_4_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_5_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_6_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_7_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_8_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_          L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_            L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_            L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_              L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_             ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_          L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_            L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_        L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_          L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_   L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_weight_   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_     L_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_LayerNorm_parameters_bias_     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_                   ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_                          L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_                         ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_                            L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_                           ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_                     L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_weight_                     ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_                       L_self_modules_encoder_modules_layer_modules_9_modules_output_modules_LayerNorm_parameters_bias_                       ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_         L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_           L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_          ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_           L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_          ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_             L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_            ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_         L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_           L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_          ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_       L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_      ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_         L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_  L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_weight_  ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_    L_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_LayerNorm_parameters_bias_    ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_                   L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_                  ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_                     L_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_                    ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_                         L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_                        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_                           L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_                          ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_weight_                    ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_10_modules_output_modules_LayerNorm_parameters_bias_                      ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_         L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_           L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_          ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_           L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_          ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_             L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_            ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_         L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_           L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_          ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_       L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_      ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_         L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_  L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_weight_  ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_    L_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_LayerNorm_parameters_bias_    ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_                   L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_                  ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_                     L_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_                    ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_                         L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_                        ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_                           L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_                          ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_                    L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_weight_                    ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_                      L_self_modules_encoder_modules_layer_modules_11_modules_output_modules_LayerNorm_parameters_bias_                      ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_pooler_modules_dense_parameters_weight_                                                                  L_self_modules_pooler_modules_dense_parameters_weight_                                                                 ()                                                                                                                                                                                                                                                                     {}\n",
            "placeholder    l_self_modules_pooler_modules_dense_parameters_bias_                                                                    L_self_modules_pooler_modules_dense_parameters_bias_                                                                   ()                                                                                                                                                                                                                                                                     {}\n",
            "call_function  position_ids                                                                                                            <built-in function getitem>                                                                                            (l_self_modules_embeddings_buffers_position_ids_, (slice(None, None, None), slice(0, 12, None)))                                                                                                                                                                       {}\n",
            "call_function  inputs_embeds                                                                                                           <function embedding at 0x7e82b4b0f0a0>                                                                                 (l_input_ids_, l_self_modules_embeddings_modules_word_embeddings_parameters_weight_, 0, None, 2.0, False, False)                                                                                                                                                       {}\n",
            "call_function  token_type_embeddings                                                                                                   <function embedding at 0x7e82b4b0f0a0>                                                                                 (l_token_type_ids_, l_self_modules_embeddings_modules_token_type_embeddings_parameters_weight_, None, None, 2.0, False, False)                                                                                                                                         {}\n",
            "call_function  embeddings                                                                                                              <built-in function add>                                                                                                (inputs_embeds, token_type_embeddings)                                                                                                                                                                                                                                 {}\n",
            "call_function  position_embeddings                                                                                                     <function embedding at 0x7e82b4b0f0a0>                                                                                 (position_ids, l_self_modules_embeddings_modules_position_embeddings_parameters_weight_, None, None, 2.0, False, False)                                                                                                                                                {}\n",
            "call_function  embeddings_1                                                                                                            <built-in function iadd>                                                                                               (embeddings, position_embeddings)                                                                                                                                                                                                                                      {}\n",
            "call_function  embeddings_2                                                                                                            <function layer_norm at 0x7e82b4b0f490>                                                                                (embeddings_1, (768,), l_self_modules_embeddings_modules_layer_norm_parameters_weight_, l_self_modules_embeddings_modules_layer_norm_parameters_bias_, 1e-12)                                                                                                          {}\n",
            "call_function  embeddings_3                                                                                                            <function dropout at 0x7e82b4b0dea0>                                                                                   (embeddings_2, 0.1, False, False)                                                                                                                                                                                                                                      {}\n",
            "call_function  getitem_1                                                                                                               <built-in function getitem>                                                                                            (l_attention_mask_, (slice(None, None, None), None, None, slice(None, None, None)))                                                                                                                                                                                    {}\n",
            "call_method    expand                                                                                                                  expand                                                                                                                 (getitem_1, 1, 1, 12, 12)                                                                                                                                                                                                                                              {}\n",
            "call_method    expanded_mask                                                                                                           to                                                                                                                     (expand, torch.float32)                                                                                                                                                                                                                                                {}\n",
            "call_function  inverted_mask                                                                                                           <built-in function sub>                                                                                                (1.0, expanded_mask)                                                                                                                                                                                                                                                   {}\n",
            "call_method    to_1                                                                                                                    to                                                                                                                     (inverted_mask, torch.bool)                                                                                                                                                                                                                                            {}\n",
            "call_method    extended_attention_mask                                                                                                 masked_fill                                                                                                            (inverted_mask, to_1, -3.4028234663852886e+38)                                                                                                                                                                                                                         {}\n",
            "call_function  linear                                                                                                                  <built-in function linear>                                                                                             (embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_query_parameters_bias_)                           {}\n",
            "call_method    x                                                                                                                       view                                                                                                                   (linear, (1, 12, 12, 64))                                                                                                                                                                                                                                              {}\n",
            "call_method    query_layer                                                                                                             permute                                                                                                                (x, 0, 2, 1, 3)                                                                                                                                                                                                                                                        {}\n",
            "call_function  linear_1                                                                                                                <built-in function linear>                                                                                             (embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_key_parameters_bias_)                               {}\n",
            "call_method    x_1                                                                                                                     view                                                                                                                   (linear_1, (1, 12, 12, 64))                                                                                                                                                                                                                                            {}\n",
            "call_method    key_layer                                                                                                               permute                                                                                                                (x_1, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  linear_2                                                                                                                <built-in function linear>                                                                                             (embeddings_3, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_self_modules_value_parameters_bias_)                           {}\n",
            "call_method    x_2                                                                                                                     view                                                                                                                   (linear_2, (1, 12, 12, 64))                                                                                                                                                                                                                                            {}\n",
            "call_method    value_layer                                                                                                             permute                                                                                                                (x_2, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  attn_output                                                                                                             <built-in function scaled_dot_product_attention>                                                                       (query_layer, key_layer, value_layer)                                                                                                                                                                                                                                  {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_1                                                                                                           transpose                                                                                                              (attn_output, 1, 2)                                                                                                                                                                                                                                                    {}\n",
            "call_method    attn_output_2                                                                                                           reshape                                                                                                                (attn_output_1, 1, 12, 768)                                                                                                                                                                                                                                            {}\n",
            "call_function  hidden_states                                                                                                           <built-in function linear>                                                                                             (attn_output_2, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_dense_parameters_bias_)                      {}\n",
            "call_function  hidden_states_1                                                                                                         <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states, 0.1, False, False)                                                                                                                                                                                                                                     {}\n",
            "call_function  add_1                                                                                                                   <built-in function add>                                                                                                (hidden_states_1, embeddings_3)                                                                                                                                                                                                                                        {}\n",
            "call_function  hidden_states_2                                                                                                         <function layer_norm at 0x7e82b4b0f490>                                                                                (add_1, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}\n",
            "call_function  hidden_states_3                                                                                                         <built-in function linear>                                                                                             (hidden_states_2, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_intermediate_modules_dense_parameters_bias_)                                            {}\n",
            "call_function  hidden_states_4                                                                                                         <built-in function gelu>                                                                                               (hidden_states_3,)                                                                                                                                                                                                                                                     {}\n",
            "call_function  hidden_states_5                                                                                                         <built-in function linear>                                                                                             (hidden_states_4, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_dense_parameters_bias_)                                                        {}\n",
            "call_function  hidden_states_6                                                                                                         <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_5, 0.1, False, False)                                                                                                                                                                                                                                   {}\n",
            "call_function  add_2                                                                                                                   <built-in function add>                                                                                                (hidden_states_6, hidden_states_2)                                                                                                                                                                                                                                     {}\n",
            "call_function  hidden_states_7                                                                                                         <function layer_norm at 0x7e82b4b0f490>                                                                                (add_2, (768,), l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_0_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                         {}\n",
            "call_function  linear_6                                                                                                                <built-in function linear>                                                                                             (hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_query_parameters_bias_)                        {}\n",
            "call_method    x_3                                                                                                                     view                                                                                                                   (linear_6, (1, 12, 12, 64))                                                                                                                                                                                                                                            {}\n",
            "call_method    query_layer_1                                                                                                           permute                                                                                                                (x_3, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  linear_7                                                                                                                <built-in function linear>                                                                                             (hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_key_parameters_bias_)                            {}\n",
            "call_method    x_4                                                                                                                     view                                                                                                                   (linear_7, (1, 12, 12, 64))                                                                                                                                                                                                                                            {}\n",
            "call_method    key_layer_1                                                                                                             permute                                                                                                                (x_4, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  linear_8                                                                                                                <built-in function linear>                                                                                             (hidden_states_7, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_self_modules_value_parameters_bias_)                        {}\n",
            "call_method    x_5                                                                                                                     view                                                                                                                   (linear_8, (1, 12, 12, 64))                                                                                                                                                                                                                                            {}\n",
            "call_method    value_layer_1                                                                                                           permute                                                                                                                (x_5, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  attn_output_3                                                                                                           <built-in function scaled_dot_product_attention>                                                                       (query_layer_1, key_layer_1, value_layer_1)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_4                                                                                                           transpose                                                                                                              (attn_output_3, 1, 2)                                                                                                                                                                                                                                                  {}\n",
            "call_method    attn_output_5                                                                                                           reshape                                                                                                                (attn_output_4, 1, 12, 768)                                                                                                                                                                                                                                            {}\n",
            "call_function  hidden_states_8                                                                                                         <built-in function linear>                                                                                             (attn_output_5, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_dense_parameters_bias_)                      {}\n",
            "call_function  hidden_states_9                                                                                                         <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_8, 0.1, False, False)                                                                                                                                                                                                                                   {}\n",
            "call_function  add_3                                                                                                                   <built-in function add>                                                                                                (hidden_states_9, hidden_states_7)                                                                                                                                                                                                                                     {}\n",
            "call_function  hidden_states_10                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_3, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}\n",
            "call_function  hidden_states_11                                                                                                        <built-in function linear>                                                                                             (hidden_states_10, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_12                                                                                                        <built-in function gelu>                                                                                               (hidden_states_11,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_13                                                                                                        <built-in function linear>                                                                                             (hidden_states_12, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_14                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_13, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_4                                                                                                                   <built-in function add>                                                                                                (hidden_states_14, hidden_states_10)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_15                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_4, (768,), l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_1_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                         {}\n",
            "call_function  linear_12                                                                                                               <built-in function linear>                                                                                             (hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_query_parameters_bias_)                       {}\n",
            "call_method    x_6                                                                                                                     view                                                                                                                   (linear_12, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_2                                                                                                           permute                                                                                                                (x_6, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  linear_13                                                                                                               <built-in function linear>                                                                                             (hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_key_parameters_bias_)                           {}\n",
            "call_method    x_7                                                                                                                     view                                                                                                                   (linear_13, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_2                                                                                                             permute                                                                                                                (x_7, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  linear_14                                                                                                               <built-in function linear>                                                                                             (hidden_states_15, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_self_modules_value_parameters_bias_)                       {}\n",
            "call_method    x_8                                                                                                                     view                                                                                                                   (linear_14, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_2                                                                                                           permute                                                                                                                (x_8, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  attn_output_6                                                                                                           <built-in function scaled_dot_product_attention>                                                                       (query_layer_2, key_layer_2, value_layer_2)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_7                                                                                                           transpose                                                                                                              (attn_output_6, 1, 2)                                                                                                                                                                                                                                                  {}\n",
            "call_method    attn_output_8                                                                                                           reshape                                                                                                                (attn_output_7, 1, 12, 768)                                                                                                                                                                                                                                            {}\n",
            "call_function  hidden_states_16                                                                                                        <built-in function linear>                                                                                             (attn_output_8, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_dense_parameters_bias_)                      {}\n",
            "call_function  hidden_states_17                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_16, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_5                                                                                                                   <built-in function add>                                                                                                (hidden_states_17, hidden_states_15)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_18                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_5, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}\n",
            "call_function  hidden_states_19                                                                                                        <built-in function linear>                                                                                             (hidden_states_18, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_20                                                                                                        <built-in function gelu>                                                                                               (hidden_states_19,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_21                                                                                                        <built-in function linear>                                                                                             (hidden_states_20, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_22                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_21, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_6                                                                                                                   <built-in function add>                                                                                                (hidden_states_22, hidden_states_18)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_23                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_6, (768,), l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_2_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                         {}\n",
            "call_function  linear_18                                                                                                               <built-in function linear>                                                                                             (hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_query_parameters_bias_)                       {}\n",
            "call_method    x_9                                                                                                                     view                                                                                                                   (linear_18, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_3                                                                                                           permute                                                                                                                (x_9, 0, 2, 1, 3)                                                                                                                                                                                                                                                      {}\n",
            "call_function  linear_19                                                                                                               <built-in function linear>                                                                                             (hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_key_parameters_bias_)                           {}\n",
            "call_method    x_10                                                                                                                    view                                                                                                                   (linear_19, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_3                                                                                                             permute                                                                                                                (x_10, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_20                                                                                                               <built-in function linear>                                                                                             (hidden_states_23, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_self_modules_value_parameters_bias_)                       {}\n",
            "call_method    x_11                                                                                                                    view                                                                                                                   (linear_20, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_3                                                                                                           permute                                                                                                                (x_11, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_9                                                                                                           <built-in function scaled_dot_product_attention>                                                                       (query_layer_3, key_layer_3, value_layer_3)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_10                                                                                                          transpose                                                                                                              (attn_output_9, 1, 2)                                                                                                                                                                                                                                                  {}\n",
            "call_method    attn_output_11                                                                                                          reshape                                                                                                                (attn_output_10, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_24                                                                                                        <built-in function linear>                                                                                             (attn_output_11, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}\n",
            "call_function  hidden_states_25                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_24, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_7                                                                                                                   <built-in function add>                                                                                                (hidden_states_25, hidden_states_23)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_26                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_7, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}\n",
            "call_function  hidden_states_27                                                                                                        <built-in function linear>                                                                                             (hidden_states_26, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_28                                                                                                        <built-in function gelu>                                                                                               (hidden_states_27,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_29                                                                                                        <built-in function linear>                                                                                             (hidden_states_28, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_30                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_29, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_8                                                                                                                   <built-in function add>                                                                                                (hidden_states_30, hidden_states_26)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_31                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_8, (768,), l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_3_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                         {}\n",
            "call_function  linear_24                                                                                                               <built-in function linear>                                                                                             (hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_query_parameters_bias_)                       {}\n",
            "call_method    x_12                                                                                                                    view                                                                                                                   (linear_24, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_4                                                                                                           permute                                                                                                                (x_12, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_25                                                                                                               <built-in function linear>                                                                                             (hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_key_parameters_bias_)                           {}\n",
            "call_method    x_13                                                                                                                    view                                                                                                                   (linear_25, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_4                                                                                                             permute                                                                                                                (x_13, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_26                                                                                                               <built-in function linear>                                                                                             (hidden_states_31, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_self_modules_value_parameters_bias_)                       {}\n",
            "call_method    x_14                                                                                                                    view                                                                                                                   (linear_26, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_4                                                                                                           permute                                                                                                                (x_14, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_12                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_4, key_layer_4, value_layer_4)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_13                                                                                                          transpose                                                                                                              (attn_output_12, 1, 2)                                                                                                                                                                                                                                                 {}\n",
            "call_method    attn_output_14                                                                                                          reshape                                                                                                                (attn_output_13, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_32                                                                                                        <built-in function linear>                                                                                             (attn_output_14, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}\n",
            "call_function  hidden_states_33                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_32, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_9                                                                                                                   <built-in function add>                                                                                                (hidden_states_33, hidden_states_31)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_34                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_9, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)     {}\n",
            "call_function  hidden_states_35                                                                                                        <built-in function linear>                                                                                             (hidden_states_34, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_36                                                                                                        <built-in function gelu>                                                                                               (hidden_states_35,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_37                                                                                                        <built-in function linear>                                                                                             (hidden_states_36, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_38                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_37, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_10                                                                                                                  <built-in function add>                                                                                                (hidden_states_38, hidden_states_34)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_39                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_10, (768,), l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_4_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}\n",
            "call_function  linear_30                                                                                                               <built-in function linear>                                                                                             (hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_query_parameters_bias_)                       {}\n",
            "call_method    x_15                                                                                                                    view                                                                                                                   (linear_30, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_5                                                                                                           permute                                                                                                                (x_15, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_31                                                                                                               <built-in function linear>                                                                                             (hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_key_parameters_bias_)                           {}\n",
            "call_method    x_16                                                                                                                    view                                                                                                                   (linear_31, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_5                                                                                                             permute                                                                                                                (x_16, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_32                                                                                                               <built-in function linear>                                                                                             (hidden_states_39, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_self_modules_value_parameters_bias_)                       {}\n",
            "call_method    x_17                                                                                                                    view                                                                                                                   (linear_32, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_5                                                                                                           permute                                                                                                                (x_17, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_15                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_5, key_layer_5, value_layer_5)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_16                                                                                                          transpose                                                                                                              (attn_output_15, 1, 2)                                                                                                                                                                                                                                                 {}\n",
            "call_method    attn_output_17                                                                                                          reshape                                                                                                                (attn_output_16, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_40                                                                                                        <built-in function linear>                                                                                             (attn_output_17, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}\n",
            "call_function  hidden_states_41                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_40, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_11                                                                                                                  <built-in function add>                                                                                                (hidden_states_41, hidden_states_39)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_42                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_11, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}\n",
            "call_function  hidden_states_43                                                                                                        <built-in function linear>                                                                                             (hidden_states_42, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_44                                                                                                        <built-in function gelu>                                                                                               (hidden_states_43,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_45                                                                                                        <built-in function linear>                                                                                             (hidden_states_44, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_46                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_45, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_12                                                                                                                  <built-in function add>                                                                                                (hidden_states_46, hidden_states_42)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_47                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_12, (768,), l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_5_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}\n",
            "call_function  linear_36                                                                                                               <built-in function linear>                                                                                             (hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_query_parameters_bias_)                       {}\n",
            "call_method    x_18                                                                                                                    view                                                                                                                   (linear_36, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_6                                                                                                           permute                                                                                                                (x_18, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_37                                                                                                               <built-in function linear>                                                                                             (hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_key_parameters_bias_)                           {}\n",
            "call_method    x_19                                                                                                                    view                                                                                                                   (linear_37, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_6                                                                                                             permute                                                                                                                (x_19, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_38                                                                                                               <built-in function linear>                                                                                             (hidden_states_47, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_self_modules_value_parameters_bias_)                       {}\n",
            "call_method    x_20                                                                                                                    view                                                                                                                   (linear_38, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_6                                                                                                           permute                                                                                                                (x_20, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_18                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_6, key_layer_6, value_layer_6)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_19                                                                                                          transpose                                                                                                              (attn_output_18, 1, 2)                                                                                                                                                                                                                                                 {}\n",
            "call_method    attn_output_20                                                                                                          reshape                                                                                                                (attn_output_19, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_48                                                                                                        <built-in function linear>                                                                                             (attn_output_20, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}\n",
            "call_function  hidden_states_49                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_48, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_13                                                                                                                  <built-in function add>                                                                                                (hidden_states_49, hidden_states_47)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_50                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_13, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}\n",
            "call_function  hidden_states_51                                                                                                        <built-in function linear>                                                                                             (hidden_states_50, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_52                                                                                                        <built-in function gelu>                                                                                               (hidden_states_51,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_53                                                                                                        <built-in function linear>                                                                                             (hidden_states_52, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_54                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_53, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_14                                                                                                                  <built-in function add>                                                                                                (hidden_states_54, hidden_states_50)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_55                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_14, (768,), l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_6_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}\n",
            "call_function  linear_42                                                                                                               <built-in function linear>                                                                                             (hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_query_parameters_bias_)                       {}\n",
            "call_method    x_21                                                                                                                    view                                                                                                                   (linear_42, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_7                                                                                                           permute                                                                                                                (x_21, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_43                                                                                                               <built-in function linear>                                                                                             (hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_key_parameters_bias_)                           {}\n",
            "call_method    x_22                                                                                                                    view                                                                                                                   (linear_43, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_7                                                                                                             permute                                                                                                                (x_22, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_44                                                                                                               <built-in function linear>                                                                                             (hidden_states_55, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_self_modules_value_parameters_bias_)                       {}\n",
            "call_method    x_23                                                                                                                    view                                                                                                                   (linear_44, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_7                                                                                                           permute                                                                                                                (x_23, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_21                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_7, key_layer_7, value_layer_7)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_22                                                                                                          transpose                                                                                                              (attn_output_21, 1, 2)                                                                                                                                                                                                                                                 {}\n",
            "call_method    attn_output_23                                                                                                          reshape                                                                                                                (attn_output_22, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_56                                                                                                        <built-in function linear>                                                                                             (attn_output_23, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}\n",
            "call_function  hidden_states_57                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_56, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_15                                                                                                                  <built-in function add>                                                                                                (hidden_states_57, hidden_states_55)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_58                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_15, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}\n",
            "call_function  hidden_states_59                                                                                                        <built-in function linear>                                                                                             (hidden_states_58, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_60                                                                                                        <built-in function gelu>                                                                                               (hidden_states_59,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_61                                                                                                        <built-in function linear>                                                                                             (hidden_states_60, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_62                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_61, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_16                                                                                                                  <built-in function add>                                                                                                (hidden_states_62, hidden_states_58)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_63                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_16, (768,), l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_7_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}\n",
            "call_function  linear_48                                                                                                               <built-in function linear>                                                                                             (hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_query_parameters_bias_)                       {}\n",
            "call_method    x_24                                                                                                                    view                                                                                                                   (linear_48, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_8                                                                                                           permute                                                                                                                (x_24, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_49                                                                                                               <built-in function linear>                                                                                             (hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_key_parameters_bias_)                           {}\n",
            "call_method    x_25                                                                                                                    view                                                                                                                   (linear_49, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_8                                                                                                             permute                                                                                                                (x_25, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_50                                                                                                               <built-in function linear>                                                                                             (hidden_states_63, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_self_modules_value_parameters_bias_)                       {}\n",
            "call_method    x_26                                                                                                                    view                                                                                                                   (linear_50, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_8                                                                                                           permute                                                                                                                (x_26, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_24                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_8, key_layer_8, value_layer_8)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_25                                                                                                          transpose                                                                                                              (attn_output_24, 1, 2)                                                                                                                                                                                                                                                 {}\n",
            "call_method    attn_output_26                                                                                                          reshape                                                                                                                (attn_output_25, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_64                                                                                                        <built-in function linear>                                                                                             (attn_output_26, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}\n",
            "call_function  hidden_states_65                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_64, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_17                                                                                                                  <built-in function add>                                                                                                (hidden_states_65, hidden_states_63)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_66                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_17, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}\n",
            "call_function  hidden_states_67                                                                                                        <built-in function linear>                                                                                             (hidden_states_66, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_68                                                                                                        <built-in function gelu>                                                                                               (hidden_states_67,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_69                                                                                                        <built-in function linear>                                                                                             (hidden_states_68, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_70                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_69, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_18                                                                                                                  <built-in function add>                                                                                                (hidden_states_70, hidden_states_66)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_71                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_18, (768,), l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_8_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}\n",
            "call_function  linear_54                                                                                                               <built-in function linear>                                                                                             (hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_query_parameters_bias_)                       {}\n",
            "call_method    x_27                                                                                                                    view                                                                                                                   (linear_54, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_9                                                                                                           permute                                                                                                                (x_27, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_55                                                                                                               <built-in function linear>                                                                                             (hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_key_parameters_bias_)                           {}\n",
            "call_method    x_28                                                                                                                    view                                                                                                                   (linear_55, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_9                                                                                                             permute                                                                                                                (x_28, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_56                                                                                                               <built-in function linear>                                                                                             (hidden_states_71, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_self_modules_value_parameters_bias_)                       {}\n",
            "call_method    x_29                                                                                                                    view                                                                                                                   (linear_56, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_9                                                                                                           permute                                                                                                                (x_29, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_27                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_9, key_layer_9, value_layer_9)                                                                                                                                                                                                                            {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_28                                                                                                          transpose                                                                                                              (attn_output_27, 1, 2)                                                                                                                                                                                                                                                 {}\n",
            "call_method    attn_output_29                                                                                                          reshape                                                                                                                (attn_output_28, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_72                                                                                                        <built-in function linear>                                                                                             (attn_output_29, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_dense_parameters_bias_)                     {}\n",
            "call_function  hidden_states_73                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_72, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_19                                                                                                                  <built-in function add>                                                                                                (hidden_states_73, hidden_states_71)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_74                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_19, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)    {}\n",
            "call_function  hidden_states_75                                                                                                        <built-in function linear>                                                                                             (hidden_states_74, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_intermediate_modules_dense_parameters_bias_)                                           {}\n",
            "call_function  hidden_states_76                                                                                                        <built-in function gelu>                                                                                               (hidden_states_75,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_77                                                                                                        <built-in function linear>                                                                                             (hidden_states_76, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_dense_parameters_bias_)                                                       {}\n",
            "call_function  hidden_states_78                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_77, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_20                                                                                                                  <built-in function add>                                                                                                (hidden_states_78, hidden_states_74)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_79                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_20, (768,), l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_9_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                        {}\n",
            "call_function  linear_60                                                                                                               <built-in function linear>                                                                                             (hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_query_parameters_bias_)                     {}\n",
            "call_method    x_30                                                                                                                    view                                                                                                                   (linear_60, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_10                                                                                                          permute                                                                                                                (x_30, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_61                                                                                                               <built-in function linear>                                                                                             (hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_key_parameters_bias_)                         {}\n",
            "call_method    x_31                                                                                                                    view                                                                                                                   (linear_61, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_10                                                                                                            permute                                                                                                                (x_31, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_62                                                                                                               <built-in function linear>                                                                                             (hidden_states_79, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_self_modules_value_parameters_bias_)                     {}\n",
            "call_method    x_32                                                                                                                    view                                                                                                                   (linear_62, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_10                                                                                                          permute                                                                                                                (x_32, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_30                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_10, key_layer_10, value_layer_10)                                                                                                                                                                                                                         {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_31                                                                                                          transpose                                                                                                              (attn_output_30, 1, 2)                                                                                                                                                                                                                                                 {}\n",
            "call_method    attn_output_32                                                                                                          reshape                                                                                                                (attn_output_31, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_80                                                                                                        <built-in function linear>                                                                                             (attn_output_32, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_dense_parameters_bias_)                   {}\n",
            "call_function  hidden_states_81                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_80, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_21                                                                                                                  <built-in function add>                                                                                                (hidden_states_81, hidden_states_79)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_82                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_21, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)  {}\n",
            "call_function  hidden_states_83                                                                                                        <built-in function linear>                                                                                             (hidden_states_82, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_intermediate_modules_dense_parameters_bias_)                                         {}\n",
            "call_function  hidden_states_84                                                                                                        <built-in function gelu>                                                                                               (hidden_states_83,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_85                                                                                                        <built-in function linear>                                                                                             (hidden_states_84, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_dense_parameters_bias_)                                                     {}\n",
            "call_function  hidden_states_86                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_85, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_22                                                                                                                  <built-in function add>                                                                                                (hidden_states_86, hidden_states_82)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_87                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_22, (768,), l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_10_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                      {}\n",
            "call_function  linear_66                                                                                                               <built-in function linear>                                                                                             (hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_query_parameters_bias_)                     {}\n",
            "call_method    x_33                                                                                                                    view                                                                                                                   (linear_66, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    query_layer_11                                                                                                          permute                                                                                                                (x_33, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_67                                                                                                               <built-in function linear>                                                                                             (hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_key_parameters_bias_)                         {}\n",
            "call_method    x_34                                                                                                                    view                                                                                                                   (linear_67, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    key_layer_11                                                                                                            permute                                                                                                                (x_34, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  linear_68                                                                                                               <built-in function linear>                                                                                             (hidden_states_87, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_self_modules_value_parameters_bias_)                     {}\n",
            "call_method    x_35                                                                                                                    view                                                                                                                   (linear_68, (1, 12, 12, 64))                                                                                                                                                                                                                                           {}\n",
            "call_method    value_layer_11                                                                                                          permute                                                                                                                (x_35, 0, 2, 1, 3)                                                                                                                                                                                                                                                     {}\n",
            "call_function  attn_output_33                                                                                                          <built-in function scaled_dot_product_attention>                                                                       (query_layer_11, key_layer_11, value_layer_11)                                                                                                                                                                                                                         {'attn_mask': extended_attention_mask, 'dropout_p': 0.0, 'is_causal': False}\n",
            "call_method    attn_output_34                                                                                                          transpose                                                                                                              (attn_output_33, 1, 2)                                                                                                                                                                                                                                                 {}\n",
            "call_method    attn_output_35                                                                                                          reshape                                                                                                                (attn_output_34, 1, 12, 768)                                                                                                                                                                                                                                           {}\n",
            "call_function  hidden_states_88                                                                                                        <built-in function linear>                                                                                             (attn_output_35, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_dense_parameters_bias_)                   {}\n",
            "call_function  hidden_states_89                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_88, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_23                                                                                                                  <built-in function add>                                                                                                (hidden_states_89, hidden_states_87)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_90                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_23, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_attention_modules_output_modules_layer_norm_parameters_bias_, 1e-12)  {}\n",
            "call_function  hidden_states_91                                                                                                        <built-in function linear>                                                                                             (hidden_states_90, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_intermediate_modules_dense_parameters_bias_)                                         {}\n",
            "call_function  hidden_states_92                                                                                                        <built-in function gelu>                                                                                               (hidden_states_91,)                                                                                                                                                                                                                                                    {}\n",
            "call_function  hidden_states_93                                                                                                        <built-in function linear>                                                                                             (hidden_states_92, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_dense_parameters_bias_)                                                     {}\n",
            "call_function  hidden_states_94                                                                                                        <function dropout at 0x7e82b4b0dea0>                                                                                   (hidden_states_93, 0.1, False, False)                                                                                                                                                                                                                                  {}\n",
            "call_function  add_24                                                                                                                  <built-in function add>                                                                                                (hidden_states_94, hidden_states_90)                                                                                                                                                                                                                                   {}\n",
            "call_function  hidden_states_95                                                                                                        <function layer_norm at 0x7e82b4b0f490>                                                                                (add_24, (768,), l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_weight_, l_self_modules_encoder_modules_layer_modules_11_modules_output_modules_layer_norm_parameters_bias_, 1e-12)                                      {}\n",
            "call_function  first_token_tensor                                                                                                      <built-in function getitem>                                                                                            (hidden_states_95, (slice(None, None, None), 0))                                                                                                                                                                                                                       {}\n",
            "call_function  pooled_output                                                                                                           <built-in function linear>                                                                                             (first_token_tensor, l_self_modules_pooler_modules_dense_parameters_weight_, l_self_modules_pooler_modules_dense_parameters_bias_)                                                                                                                                     {}\n",
            "call_function  pooled_output_1                                                                                                         <built-in method tanh of type object at 0x7e82b678d4a0>                                                                (pooled_output,)                                                                                                                                                                                                                                                       {}\n",
            "output         output                                                                                                                  output                                                                                                                 ((hidden_states_95, pooled_output_1),)                                                                                                                                                                                                                                 {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKdk98UburJs",
        "outputId": "f54da4de-225a-4981-ff9c-3e0bb2042662"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with depyf.prepare_debug(\"depyf_debug_dir\"):\n",
        "  output = model(**encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs3NFUt5qWwB",
        "outputId": "2f5840e0-57c9-4d48-d9cc-fb3a1f17db9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ":0: UserWarning: /usr/local/lib/python3.10/dist-packages/depyf/explain/enable_debugging.py:163: You are trying to debug `torch.compile`. Please make sure the code runs multiple times to cover all the possible branches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/depyf_debug.zip /content/depyf_debug_dir/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI7uCt0is0ux",
        "outputId": "72e1c3a4-7e94-45ba-c187-ff8b37f1eebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/depyf_debug_dir/ (stored 0%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_9.Forward_graph.0.py (deflated 68%)\n",
            "  adding: content/depyf_debug_dir/full_code_for_forward_2.py (deflated 68%)\n",
            "  adding: content/depyf_debug_dir/full_code_for_forward_1.py (deflated 92%)\n",
            "  adding: content/depyf_debug_dir/__transformed_code_2_for_forward.py (deflated 33%)\n",
            "  adding: content/depyf_debug_dir/full_code_for_forward_0.py (deflated 74%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_7.AFTER_POST_GRAD.0.py (deflated 76%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_9.Captured_Graph.0.py (deflated 66%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_14.Joint_graph.0.py (deflated 76%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_9.AFTER_POST_GRAD.0.py (deflated 66%)\n",
            "  adding: content/depyf_debug_dir/__transformed_code_0_for_warn_if_padding_and_no_attention_mask.py (deflated 31%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_7.Captured_Graph.0.py (deflated 80%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_14.AFTER_POST_GRAD.0.py (deflated 70%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_14.kernel_0.py (deflated 64%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_14.Captured_Graph.0.py (deflated 67%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_12.AFTER_POST_GRAD.0.py (deflated 91%)\n",
            "  adding: content/depyf_debug_dir/full_code_for_warn_if_padding_and_no_attention_mask_0.py (deflated 80%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_9.kernel_0.py (deflated 66%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_7.Backward_graph.0.py (deflated 78%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_12.Backward_graph.0.py (deflated 90%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_7.kernel_0.py (deflated 79%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_12.kernel_0.py (deflated 95%)\n",
            "  adding: content/depyf_debug_dir/__transformed_code_0_for__prepare_4d_attention_mask_for_sdpa.py (deflated 33%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_12.Captured_Graph.0.py (deflated 94%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_14.Forward_graph.0.py (deflated 70%)\n",
            "  adding: content/depyf_debug_dir/__transformed_code_1_for_forward.py (deflated 52%)\n",
            "  adding: content/depyf_debug_dir/__transformed_code_0_for_forward.py (deflated 49%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_14.Backward_graph.0.py (deflated 69%)\n",
            "  adding: content/depyf_debug_dir/full_code_for__prepare_4d_attention_mask_for_sdpa_0.py (deflated 70%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_7.Forward_graph.0.py (deflated 76%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_7.Joint_graph.0.py (deflated 82%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_12.Joint_graph.0.py (deflated 90%)\n",
            "  adding: content/depyf_debug_dir/__compiled_fn_12.Forward_graph.0.py (deflated 91%)\n"
          ]
        }
      ]
    }
  ]
}